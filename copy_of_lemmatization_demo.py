# -*- coding: utf-8 -*-
"""Copy of lemmatization_demo.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sORrUbDU68Fzpavq0VaeuSyHWtvhIMNH

# Tokenization
"""

import nltk
nltk.download('wordnet')

text = "This is Andrew's text, isn't it?"

tokenizer = nltk.tokenize.WhitespaceTokenizer()
tokenizer.tokenize(text)

tokenizer = nltk.tokenize.TreebankWordTokenizer()
tokenizer.tokenize(text)

tokenizer = nltk.tokenize.WordPunctTokenizer()
tokenizer.tokenize(text)

"""# Stemming (further in the video)"""

text = "feet wolves cats talked"
tokenizer = nltk.tokenize.TreebankWordTokenizer()
tokens = tokenizer.tokenize(text)

stemmer = nltk.stem.PorterStemmer()
" ".join(stemmer.stem(token) for token in tokens)

stemmer = nltk.stem.WordNetLemmatizer()
" ".join(stemmer.lemmatize(token) for token in tokens)